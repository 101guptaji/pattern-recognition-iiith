<!DOCTYPE html>
<head></head>
<h1 id="main-title">
Feature Representation
</h1>

<div id="introduction">
    <h2>Introduction </h2>
    <p> The respresentation of an object in computer plays the most critical role in our
    ability to distinguish it as a member of a class, and differentiate it from samples of other
    classes. In this experiment, we will carry out a sequence of tasks that reveals the nature
    of different features and its effect on the feature space representation. You will also be
    able to craft your own features and see how it affects the representation and the classification
    performance.</p>
    
     <p>Objects in real world are complex multi-dimensional entities, characterized
     by their shape, color, texture, weight, chemical and physical composition,
     etc. In a feature representation, each object is reduced to a specific set of
     measurements that the designer deems to be important for the purpose of solving
     the (classification) problem. In this experiment, we will explore the use of
     various features for representation of objects in a two-dimensional feature space.</p>
</div>
<div id="theory">
    <h2>Theory</h2>
    <p>Feature extraction allows one to convert a complex object in real world to a
    more tractable sequence of numbers; or a <i>feature vector</i>. The goal of feature
    extraction in the context of pattern classification is to capture those properties of 
    a class that makes them different from other classes under consideration.
    </p>
    <h3>Types of Feature Vectors</h3>
    <p> Depending on the nature of features that are extracted, the representation could
    be either <i>Fixed Length</i> or <i> Variable Length</i>. In a fixed length representation,
    every object of every class will be represented using a fixed set of numbers. For example,
    for classification of different fruits, each fruit can be represented using the vector:
    <i>(diameter,redness,weight)</i>. However, in certain problems, it is more appropriate not
    to restrict the representation to be fixed length. For example, in the problem of speech
    recognition, the representation of a single word could very in length depending on the speed
    at which the word is pronounced or the accent of pronounciation.
    </p>
    <p> Fixed length representations are more popular due to the ease of comparing different
    samples. Note that fixed length feature vectors could be thought of as points in a 
    <i>d</i>-dimensional vector space, commonly referred to as a <i>feature space</i>.
    We will concentrate on fixed length representations in this experiment.</p>

    <h3>Nearest Neighbor Classifier</h3>
    <p>The <i>Euclidean distance</i> between two points in the feature space can be thought of as a
    measure of dissimilarity between the samples. This fact is used for decision making in the
     popular <i>Nearest Neighbor</i> classifier. A set of labeled samples that are obtained during
     training is stored in the feature space as reference points. When an new sample whose class is
     unknown is obtained, we compare it with each of the reference points in the feature space. 
     The closest reference point is the most similar, and hence its class label is assigned to the 
     unknown sample. Even though the method is very simple, it is surprisingly effective if the
     feature representation is good. For this reason, the performance nearest neighbor classifier 
     is often used to compare different feature representations. In this experiment, we will use
     the accuracy of a nearest neighbor classifier to compare the performances of different feature
     representations.
    </p>
    <p> A variant of the nearest neighbor algorithm is the k-nearest neighbor algorithm, where you
    find the class labels of the k nearest reference samples for a new test sample. The label of the
    test sample is decided as the majority label among the reference samples. One could also weight the
    labels according their similarity (closer samples getting higher weight).
    </p>
    <h3>Other factors affecting the representation</h3>
    <p><b>Nature of Features</b>: Another factor that affects the distribution of samples in the 
    feature space is the nature of the features extracted. Some of the features could be binary valued,
    others discrete, while some others could be continuous valued.</p>
    <p><b>Scale of Features</b>: If we have two features, one varying in the range [0,1]$, while the
    second varies in [0,100], then any difference in the second will dominate the distance computation
    between two samples. This effectively makes the first feature, redundant. It is important to make
    sure that the features are computed in similar scales (of numbers) for the representation to be
    effective.</p>
</div>

<div id="objective">
    <h2> Objective </h2>
    <p>The high level goals of the experiment are:
        <ol>
            <li> <b>1:</b> To understand the role of feature extraction as a filter of information and the effect of this
                filtering process on our ability to classify.</li>
            <li> <b>2:</b> To understand the effect of various types of features on the distribution of points in feature space.</li>
            <li> <b>3:</b> To generate your own feature set by combining existing set of features, or defining new ones.</li>
            <li> <b>4:</b> To appreciate the appropriateness of different feature sets to different classification problems.</li>
        </ol>
    </p>
</div>


<div id="procedure">
    <h2> Procedure </h2>
    <p>
    Note the structure of the lab: The first pane allows you to select a dataset from a list of
    available ones. Each dataset has a set of features that have been extracted for your use. You
    are required to select or create two features for each dataset such that resultant feature space
    representation is <i>effective</i>. You can visualize the data in the feature space, once you
    define the features. You can also check the accuracy of the resulting nearest neighbor classifier
    on a set of test samples.</p>
    <h3>Stage 1:</h3>
    <ol>
        <li>Load the 0-1 dataset and select the first two features. </li>
        <li> Display the samples in a 2-d feature space</li>
        <li> Click the classify button to check the accuracy of the NN classifier for the chosen representation.</li>
        <li> Note the target accuracy for the experiment. Modify the features to achieve the target accuracy.
    </ol>
    Note: You can get a complete view of the data samples and the corresponding features below the experiment pane.
    Plot different features to get an idea of the nature of different features. Derive new features based on this
    by combining multiple features using addition or multiplication.

    <h3>Stage 2:</h3>
    <p> Repeat the above procedure for the different datasets given in the experiment. In each case, note the
    target accuracy and try to achieve it. Note that in some cases, you may not be able to cross the target
    accuracy using the provided features only (see next stage).</p>

    <h3>Stage 3:</h3>
    <p> Read the instructions on how to extract your own features for the datasets. Extract the features
    using your favorite programming environment, and dump the features into an xml file in the specified
    format. Use these new features that you created to achieve the target accuracy in cases where you were
    not able to do with the features provided.</p>

</div>

<div id="experiment">
    <h2> Experiment </h2>
    <a href="../experiment/Exp1.jnlp">Click here to open the Experiment</a>
</div>


<div id="quizzes">
    <h2>Quiz</h2>
    <p>
    <b>Q1.</b> What is the effect of correlation between two features on the feature space representation? <br/><br/>
    <b>Q2.</b> Is it possible that the addition of a new feature decrease the accuracy of the NN classifier? How? <br/><br/>
    <b>Q3.</b> Is it correct that for nearest neighbor classifier, it is always best to have all features scaled
    to the same range? Show why or give a counter example. <br/><br/>
    <b>Q4.</b> Can the nearest neighbor classifier be applied to feature vectors of different length?
    Is it possible to do so if you have a function that takes in two feature vectors (possibly of different lengths),
    and outputs the similarity/distance between them? Suggest a possible function of this nature, and a classification
    problem, where it can be applied. <br/><br/>
    <input type="submit" value="Submit Answers">
    </p>
</div>

<div id="readings">
    <h2> Further Readings </h2>
    <ul>
        <li><b>*</b> <a href="http://clopinet.com/fextract-book/">Feature Extraction, Foundations and Applications</a>,
        by  Isabelle Guyon, Steve Gunn, Masoud Nikravesh, and Lofti Zadeh, (ed). Series Studies in Fuzziness
        and Soft Computing, Physica-Verlag, Springer, 2006.
        </li>
        <li><b>*</b> Article on <a href="http://en.wikipedia.org/wiki/Feature_extraction">
        Feature Extraction</a> at Wikipedia
        </li>
        <li><b>*</b> Article on the <a href="http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">
        K-Nearest Neighbor algorithm</a> at Wikipedia
        </li>

    </ul>
</div>

